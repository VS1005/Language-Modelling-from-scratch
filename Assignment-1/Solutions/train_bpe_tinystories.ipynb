{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a06624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import psutil\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f8ba85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "  Downloading regex-2025.9.18-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Downloading regex-2025.9.18-cp39-cp39-macosx_11_0_arm64.whl (286 kB)\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2025.9.18\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install regex\n",
    "import regex as re\n",
    "\n",
    "def train_bpe(input_path:str, vocab_size:int, special_tokens:list[str])->tuple[dict[int,bytes], list[tuple[bytes, bytes]]]:\n",
    "    with open(input_path, 'rb') as f:\n",
    "        text_bytes=f.read()\n",
    "    text_str=text_bytes.decode('utf-8')\n",
    "\n",
    "    PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "    words=re.findall(PAT, text_str)\n",
    "\n",
    "    from collections import Counter\n",
    "    word_freq=Counter(words)\n",
    "    \n",
    "    word_tokens={}\n",
    "    for word, freq in word_freq.items():\n",
    "        word_bytes=word.encode('utf-8')\n",
    "        token_seq=tuple(word_bytes) # \"abs\" -> (64,37,37)\n",
    "        word_tokens[token_seq]=freq \n",
    "\n",
    "    vocab={}\n",
    "    for i in range(256):\n",
    "        vocab[i]=bytes([i])\n",
    "    nxt_tid=256\n",
    "    for x in special_tokens:\n",
    "        vocab[nxt_tid]=x.encode('utf-8')\n",
    "        nxt_tid+=1\n",
    "\n",
    "    def cnt_pairs(word_tokens):\n",
    "        pair_cnt=Counter()\n",
    "        for token_seq, freq in word_tokens.items():\n",
    "            for i in range(len(token_seq)-1):\n",
    "                pair=(token_seq[i], token_seq[i+1])\n",
    "                pair_cnt[pair]+=freq\n",
    "        return pair_cnt\n",
    "    \n",
    "    def mfr_pair(pair_cnt, word_tokens):\n",
    "        if not pair_cnt:\n",
    "            return None\n",
    "        max_freq = max(pair_cnt.values())\n",
    "        most_freq_pairs = [pair for pair, freq in pair_cnt.items() if freq == max_freq]\n",
    "        return max(most_freq_pairs, key=lambda p: (vocab[p[0]], vocab[p[1]])) #This was the error that gpt resolved\n",
    "    \n",
    "    def merge(word_tokens, pair, new_token_id):\n",
    "        new_word_tokens={}\n",
    "        tk1, tk2=pair\n",
    "        for token_seq, freq in word_tokens.items():\n",
    "            new_seq=[]\n",
    "            i=0\n",
    "            while i<len(token_seq):\n",
    "                if(i<len(token_seq)-1 and token_seq[i]==tk1 and token_seq[i+1]==tk2):\n",
    "                    new_seq.append(new_token_id)\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_seq.append(token_seq[i])\n",
    "                    i+=1\n",
    "\n",
    "            new_word_tokens[tuple(new_seq)]=freq\n",
    "        return new_word_tokens\n",
    "    \n",
    "    merges=[]\n",
    "    while len(vocab)<vocab_size :\n",
    "        pair_count=cnt_pairs(word_tokens)\n",
    "        mfp=mfr_pair(pair_count, word_tokens)\n",
    "        if mfp is None:\n",
    "            break\n",
    "        tk1_bytes=vocab[mfp[0]]\n",
    "        tk2_bytes=vocab[mfp[1]]\n",
    "        new_token_id=nxt_tid\n",
    "        nxt_tid+=1\n",
    "        vocab[new_token_id]=tk1_bytes+tk2_bytes\n",
    "        merges.append((tk1_bytes, tk2_bytes))\n",
    "        word_tokens=merge(word_tokens, mfp, new_token_id)\n",
    "\n",
    "    return vocab, merges\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc41764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mem_gb():\n",
    "    process=psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024**3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "675cb8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vo_merges(vocab, merges, output_dir=\"tokenizer_output\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    import base64\n",
    "    vocab_serial={\n",
    "        str(k): base64.b64encode(v).decode('utf-8') for k,v in vocab.items()\n",
    "    }\n",
    "    vocab_path=os.path.join(output_dir, \"vocab.json\")\n",
    "    with open(vocab_path, 'w') as f:\n",
    "        json.dump(vocab_serial, f, indent=2, sort_keys=True)\n",
    "    print(f\" Vocabulary saved to : {vocab_path} \")\n",
    "    merges_path=os.path.join(output_dir, \"merges.txt\")\n",
    "    with open(merges_path, 'w') as f:\n",
    "        for tk1, tk2 in merges:\n",
    "            try:\n",
    "                tk1_str=tk1.decode('utf-8')\n",
    "                tk2_str=tk2.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                tk1_str=repr(tk1)\n",
    "                tk2_str=repr(tk2)\n",
    "            f.write(f\"{tk1_str} {tk2_str}\\n\")\n",
    "    print(f\" Merges saved to : {merges_path} \")\n",
    "\n",
    "    vocab_read_path=os.path.join(output_dir, \"vocab_read.txt\")\n",
    "    with open(vocab_read_path, 'w', encoding='utf-8') as f:\n",
    "        for tk_id, tk_bytes in sorted(vocab.items()):\n",
    "            try:\n",
    "                tk_str=tk_bytes.decode('utf-8')\n",
    "            except UnicodeDecodeError:\n",
    "                tk_str=repr(tk_bytes)\n",
    "            f.write(f\"{tk_id}\\t{tk_str}\\n\")\n",
    "    print(f\" Vocabulary (readable) saved to : {vocab_read_path} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23211af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset_path=\"/Users/vsingh/Documents/CS336 Tatsu/data/TinyStoriesV2-GPT4-train.txt\"\n",
    "    vocab_size=10000\n",
    "    special_tokens=[\"<|endoftext|>\"]\n",
    "    output_dir=\"tinystories_tokenizer\"\n",
    "\n",
    "    print(\"TRAINING BPE TOKENIZER ON TINYSTORIES\")\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\" File not found: {dataset_path}\")\n",
    "        print(\"\\nPlease update dataset_path to point to your TinyStories dataset.\")\n",
    "        print(\"Common locations:\")\n",
    "        print(\"  - ./TinyStories.txt\")\n",
    "        print(\"  - ./data/TinyStories.txt\")\n",
    "        print(\"  - ~/datasets/TinyStories.txt\")\n",
    "        return\n",
    "    \n",
    "    initial_memory = get_mem_gb()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING STARTED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Initial memory usage: {initial_memory:.2f} GB\")\n",
    "    print(f\"Start time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "\n",
    "    vocab, merges = train_bpe(\n",
    "        input_path=dataset_path,\n",
    "        vocab_size=vocab_size,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "\n",
    "    end_time = time.time()\n",
    "    final_memory = get_mem_gb()\n",
    "    peak_memory = final_memory  # Note: This is current, not peak\n",
    "    \n",
    "    training_time_seconds = end_time - start_time\n",
    "    training_time_hours = training_time_seconds / 3600\n",
    "    training_time_minutes = training_time_seconds / 60\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"End time: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Training time: {training_time_hours:.2f} hours ({training_time_minutes:.2f} minutes)\")\n",
    "    print(f\"Final memory usage: {final_memory:.2f} GB\")\n",
    "    print(f\"Memory increase: {final_memory - initial_memory:.2f} GB\")\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    save_vo_merges(vocab, merges, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3339abcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING BPE TOKENIZER ON TINYSTORIES\n",
      "\n",
      "============================================================\n",
      "TRAINING STARTED\n",
      "============================================================\n",
      "Initial memory usage: 0.95 GB\n",
      "Start time: 2025-10-04 22:13:38\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETED\n",
      "============================================================\n",
      "End time: 2025-10-04 22:52:43\n",
      "Training time: 0.65 hours (39.09 minutes)\n",
      "Final memory usage: 0.03 GB\n",
      "Memory increase: -0.91 GB\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS\n",
      "============================================================\n",
      " Vocabulary saved to : tinystories_tokenizer/vocab.json \n",
      " Merges saved to : tinystories_tokenizer/merges.txt \n",
      " Vocabulary (readable) saved to : tinystories_tokenizer/vocab_read.txt \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a60ef13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd05ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cfafb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
